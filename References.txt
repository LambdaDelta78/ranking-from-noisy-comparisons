References
[1] Jamieson, K., Malloy, M., Nowak, R., & Bubeck, S. (2014, May). lil’ucb: An optimal exploration algorithm for multi-armed bandits. In Conference on Learning Theory (pp. 423-439).
[2] Feige, U., Raghavan, P., Peleg, D., & Upfal, E. (1994). Computing with noisy information. SIAM Journal on Computing, 23(5), 1001-1018.
[3] Ren, W., Liu, J., & Shroff, N. B. (2019). On Sample Complexity Upper and Lower Bounds for Exact Ranking from Noisy Comparisons. arXiv preprint arXiv:1909.03194.
[4] Kaufmann, E., & Kalyanakrishnan, S. (2013, June). Information complexity in bandit subset selection. In Conference on Learning Theory (pp. 228-251).
[5] Szörényi, B., Busa-Fekete, R., Paul, A., & Hüllermeier, E. (2015). Online rank elicitation for plackett-luce: A dueling bandits approach. In Advances in Neural Information Processing Systems (pp. 604-612).
[6] Heckel, R., Shah, N. B., Ramchandran, K., & Wainwright, M. J. (2016). Active ranking from pairwise comparisons and when parametric assumptions don't help. arXiv preprint arXiv:1606.08842.
